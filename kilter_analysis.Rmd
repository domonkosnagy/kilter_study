---
title: "kilter_analysis"
output: html_document
date: "2025-11-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(lme4)
library(dplyr)
library(tidyr)
library(car)
library(afex)
library(emmeans)
library(effectsize)

data <- read.csv("kilter_data.csv")
```

```{r results='hide', include=FALSE}}
#rename pd_post and pd_pre values 1-5
data$pd_pre <- recode(data$pd_pre, 
              "'Very easy' = 1; 'Easy' = 2; 'Fair' = 3; 'Hard' = 4; 'Very hard' = 5")
data$pd_post <- recode(data$pd_post, 
              "'Very easy' = 1; 'Easy' = 2; 'Fair' = 3; 'Hard' = 4; 'Very hard' = 5")

data$ID <- factor(data$ID)
data$condition <- factor(data$condition)
data$label <- factor(data$label)
data$climbing_experience <- factor(data$climbing_experience)
data$kilterboard_experience <- factor(data$kilterboard_experience)
data$boulders_grade <- factor(data$boulders_grade)
data$kilterboard_grade <- factor(data$kilterboard_grade)
data$hangboard_grade <- factor(data$hangboard_grade)
data$baseline_grade <- factor(data$baseline_grade)
data$tries <- factor(data$tries)
data$success <- as.logical(data$success)
data$pd_pre <- as.numeric(data$pd_pre)
data$pd_post <- as.numeric(data$pd_post)

#rename true and false as 1 and 0
data$success <- ifelse(data$success == TRUE, 1, 0)
```

```{r}
data$group <- ifelse(data$baseline %in% c("V0", "V1", "V2", "V3"),
                     "Beginner",
                     "Expert")

data$kilter_experience_group <- ifelse(data$kilterboard_experience %in% c("Never tried it"),
                                       "Beginner",
                                       "Expert")

data_filter <- data %>%
  group_by(ID) %>%
  mutate(
    all_succeeded = all(success == TRUE, na.rm = TRUE),
    all_failed    = all(success == FALSE, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  filter(!(all_succeeded | all_failed)) %>%
  select(-all_succeeded, -all_failed)
```


# DESCRIPTIVE STATISTICS
```{r}
data_filter %>% 
  group_by(label) %>% 
  summarise(
    n = n(),
    mean_success = mean(success),
    mean_pd_pre = mean(pd_pre),
    mean_pd_post = mean(pd_post),
    sd_pd_pre = sd(pd_pre),
    sd_pd_post = sd(pd_post)
  )

data_filter %>% 
  group_by(climbing_experience) %>%
  summarise(count = n()/3)

data_filter %>% 
  group_by(kilterboard_experience) %>%
  summarise(count = n()/3)

data_filter %>%
  group_by(group) %>%
  summarise(count = n()/3)

grade_summary <- data_filter %>%
  group_by(baseline_grade) %>%
  summarise(count = n() / 3)

# barplot of baseline grades
grade_summary$baseline_grade <- factor(
  grade_summary$baseline_grade,
  levels = c("V0", "V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V11")
)

ggplot(grade_summary, aes(x = baseline_grade, y = count)) +
  geom_col(fill = "lightblue", color = "black") +
  
  # Vertical boundary line between V3 and V4
  geom_vline(aes(xintercept = 4.5, linetype = "Beginner–Expert boundary"),
             color = "red", size = 1.5) +

  scale_linetype_manual(name = "",
                        values = c("Beginner–Expert boundary" = "solid")) +
  
  labs(
    title = "Distribution of Baseline Grades (Adjusted Count)",
    x = "Baseline Grade",
    y = "Count per Participant (n/3)"
  ) +
  ylim(0, 11.5) +
  theme_minimal() +
  theme(
    # Legend inside top-right
    legend.position = c(0.98, 0.98),
    legend.justification = c("right", "top"),
    
    # Smaller legend box
    legend.background = element_rect(fill = "white", color = "black", size = 0.1),
    legend.margin = margin(0, 2, 0, 2),
    legend.key.size = unit(10, "pt"),
    legend.spacing = unit(1, "pt"),
    legend.text = element_text(size = 10)
  )
```

# PERCEIVED DIFFICULTY ANALYSIS
```{r, H.1.1}
#Is there a significant difference in perceived difficulty between labels?
group_by(data_filter, label) %>% 
  summarise(
    mean_pd_pre = mean(as.numeric(pd_pre), na.rm = TRUE),
    sd_pd_pre = sd(as.numeric(pd_pre), na.rm = TRUE),
    mean_pd_post = mean(as.numeric(pd_post), na.rm = TRUE),
    sd_pd_post = sd(as.numeric(pd_post), na.rm = TRUE)
)
#ANOVA
anova1 <- aov_ez(
  id = "ID",      
  dv = "pd_pre",          
  data = data_filter,
  within = "label"
)
summary(anova1)

anova2 <- aov_ez(
  id = "ID",      
  dv = "pd_post",          
  data = data_filter,
  within = "label"
)
summary(anova2)

# effect sizes
eta_squared(anova1, partial = FALSE)
eta_squared(anova2, partial = FALSE)

# pairwise comparisons
emmeans(anova1, pairwise ~ label, adjust = "bonferroni")
emmeans(anova2, pairwise ~ label, adjust = "bonferroni")

# long format for faceting
data_long <- data %>%
  pivot_longer(
    cols = c(pd_pre, pd_post),
    names_to = "Time",
    values_to = "PD"
  ) %>%
  mutate(Time = factor(Time, levels = c("pd_pre", "pd_post"),
                       labels = c("Pre", "Post")))

ggplot(data_long, aes(x = label, y = PD, fill = label)) +
  geom_violin(trim = TRUE, alpha = 0.5) +
  stat_summary(
    fun = mean,
    geom = "point",
    shape = 21,   # circle with fill
    size = 3,
    color = "black",
    fill = "white"
  ) +
  facet_wrap(~Time, scales = "free_y") +
  labs(
    title = "Perceived Difficulty by Label (Pre vs Post)",
    x = "Label",
    y = "Perceived Difficulty"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

# bar plot
ggplot(data_long, aes(x = label, y = PD, fill = label)) +
  geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.9)) +
  geom_errorbar(stat = "summary", fun.data = mean_se, 
                position = position_dodge(width = 0.8), width = 0.2) +
  facet_wrap(~Time, scales = "free_y") +
  labs(
    title = "Mean Perceived Difficulty by Label (Pre vs Post)",
    x = "Label",
    y = "Perceived Difficulty"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```


```{r}
#assumptions
#normality
shapiro.test(residuals(anova1)) 
shapiro.test(residuals(anova2))
# so these actually fail because of Likert data but the rest is fine 
# "Although the Shapiro–Wilk test was significant, this outcome is expected with 1–5 Likert scales, which constrain values to a small set of discrete categories. Visual inspection of Q–Q plots and histograms showed that the residuals were approximately normally distributed without substantial skew or outliers. Levene’s test was non-significant, F(df1, df2) = XX, p > .05, indicating homogeneity of variances. Therefore, the ANOVA results were considered valid."

qqnorm(residuals(anova1)); qqline(residuals(anova1))
qqnorm(residuals(anova2)); qqline(residuals(anova2))

hist(residuals(anova1))
hist(residuals(anova2))

#homogeneity of variance
leveneTest(pd_pre ~ label, data = data)
leveneTest(pd_post ~ label, data = data)
```


```{r, H.1.2}
# ANOVA for interaction between expertise and conditions. <- significant group effect for pd_pre

#means
group_by(data_filter, label, group) %>%
  summarise(
    mean_pd_pre = mean(pd_pre, na.rm = TRUE),
    sd_pd_pre = sd(pd_pre, na.rm = TRUE),
    mean_pd_post = mean(pd_post, na.rm = TRUE),
    sd_pd_post = sd(pd_post, na.rm = TRUE),
    .groups = "drop"
  )

anova3 <- aov_ez(
  id = "ID",         
  dv = "pd_pre",          
  data = data_filter,
  within = "label",
  between = "group"
)
summary(anova3)

anova4 <- aov_ez(
  id = "ID",         
  dv = "pd_post",          
  data = data_filter,
  within = "label",
  between = "group"
)
summary(anova4)

# effect sizes
eta_squared(anova3, partial = FALSE)
eta_squared(anova4, partial = FALSE)

# pairwise comparisons
emmeans(anova3, pairwise ~ label * group, adjust = "bonferroni")
emmeans(anova4, pairwise ~ label * group, adjust = "bonferroni")

data_long2 <- data_filter %>%
  pivot_longer(
    cols = c(pd_pre, pd_post),
    names_to = "Time",
    values_to = "PD"
  ) %>%
  mutate(Time = factor(Time, levels = c("pd_pre", "pd_post"),
                       labels = c("Pre", "Post")))
#plot
ggplot(data_long2, aes(x = label, y = PD, fill = group)) +
  geom_bar(stat = "summary", fun = "mean", position = position_dodge(width = 0.9), alpha = 0.7) +
  geom_errorbar(stat = "summary", fun.data = mean_se, 
                position = position_dodge(width = 0.8), width = 0.2) +
  facet_wrap(~Time, scales = "free_y") +
  labs(
    title = "Mean Perceived Difficulty by Label and Group (Pre vs Post)",
    x = "Label",
    y = "Perceived Difficulty"
  ) +
  theme_minimal()
```


```{r}
# differences in perceived difficulty (pd_post - pd_pre)
data_filter <- data_filter %>%
  mutate(pd_difference = pd_post - pd_pre)

# absolute differences
group_by(data_filter, group) %>%
  summarise(
    mean_abs_difference = mean(abs(pd_difference), na.rm = TRUE),
    sd_abs_difference = sd(abs(pd_difference), na.rm = TRUE),
    .groups = "drop"
  )

# t-test for absolute differences between groups
data_filter <- data_filter %>%
  mutate(abs_pd_difference = abs(pd_post - pd_pre))
t_test_result <- t.test(abs_pd_difference ~ group, data = data_filter)
t_test_result
```

```{r}
#anova assumptions
shapiro.test(residuals(anova3))
shapiro.test(residuals(anova4))
qqnorm(residuals(anova3)); qqline(residuals(anova3))
qqnorm(residuals(anova4)); qqline(residuals(anova4))
hist(residuals(anova3))
hist(residuals(anova4))
leveneTest(pd_pre ~ label * group, data = data)
leveneTest(pd_post ~ label * group, data = data)

#t-test assumptions 
shapiro.test(data_filter$abs_pd_difference[data_filter$group == "Beginner"])
shapiro.test(data_filter$abs_pd_difference[data_filter$group == "Expert"])
qqnorm(data_filter$abs_pd_difference[data_filter$group == "Beginner"]);
qqline(data_filter$abs_pd_difference[data_filter$group == "Beginner"])
qqnorm(data_filter$abs_pd_difference[data_filter$group == "Expert"]);
qqline(data_filter$abs_pd_difference[data_filter$group == "Expert"])
hist(data_filter$abs_pd_difference[data_filter$group == "Beginner"])
hist(data_filter$abs_pd_difference[data_filter$group == "Expert"])
leveneTest(abs_pd_difference ~ group, data = data_filter)
```

# OBJECTIVE SUCCESS ANALYSIS 
# LABEL
```{r, H.2.1}
#successful attempts by label
data_filter %>%
  group_by(label) %>%
  summarise(
    mean_success = mean(success),
    .groups = "drop"
  )

m1 <- glm(success ~ label, data = data_filter, family = binomial)
summary(m1)

#convert coefficients to probabilities 
pred_log_odds <- coef(m1)
pred_probabilities <- plogis(pred_log_odds)
pred_probabilities

ggplot(data_filter, aes(label, success, fill = label)) +
  stat_summary(fun = mean, geom = "bar") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", color = "black", width = .2) +
  ylim (0, 1) +
  theme_minimal() +
  labs(
    title = "Successful Ascents by Label",
    x = "Label",
    y = "Proportion of Successful Attempts"
  ) +
  theme(legend.position = "none")
```


# LABEL AND GROUP INTERACTION
```{r, H.2.2}
#successful attempts by label and group
data_filter %>%
  group_by(label, group) %>%
  summarise(
    mean_success = mean(success),
    .groups = "drop"
  )

m2 <- glm(success ~ label * group, data = data_filter, family = binomial)
summary(m2)

#m2b <- glm(success ~ label * kilter_experience_group, data = data_filter, family = binomial)
#summary(m2b)

# box plot
ggplot(data_filter, aes(x = label, y = success, color = group)) +
  geom_jitter(width = 0.1, height = 0, alpha = 0.5, size = 2) +
  stat_summary(fun = mean, geom = "point", position = position_dodge(width = 0.2), size = 3) +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", 
               position = position_dodge(width = 0.2), width = 0.2, linewidth = 0.5) +
  labs(
    title = "Successful Ascents by Label and Group",
    x = "Label",
    y = "Proportion of Successful Attempts",
    color = "Group"
  ) +
  ylim(0, 1) +
  theme_minimal()
```

# PERCEIVED DIFFICULTY ON OBJECTIVE PERFORMANCE
```{r, H.1.3}
#successful attempts by pd
pd_pre_summary <- data_filter %>%
  group_by(pd_pre) %>%
  summarise(mean_success = mean(success), .groups = "drop") %>%
  mutate(time = "pre", pd = pd_pre) %>%
  select(pd, time, mean_success)
pd_pre_summary

pd_post_summary <- data_filter %>%
  group_by(pd_post) %>%
  summarise(mean_success = mean(success), .groups = "drop") %>%
  mutate(time = "post", pd = pd_post) %>%
  select(pd, time, mean_success)
pd_post_summary

# combine
pd_both <- bind_rows(pd_pre_summary, pd_post_summary)

ggplot(pd_both, aes(x = pd, y = mean_success, color = time)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:5) +
  ylim(0, 1) +
  labs(
    title = "Successful Ascents by Perceived Difficulty",
    x = "Perceived Difficulty",
    y = "Proportion of Successful Attempts",
    color = "Time"
  ) +
  theme_minimal()

```

```{r}
m3 <- glm(success ~ pd_pre, data = data_filter, family = binomial)
summary(m3)
m4 <- glm(success ~ pd_post, data = data_filter, family = binomial)
summary(m4) # these 2 are kinda cool, pd_post has significant effect, pre does not

# plot both
# pd_values <- data.frame(pd = 1:5)
# predictions_pre <- predict(m3, newdata = data.frame(pd_pre = pd_values$pd), type = "response")
# predictions_post <- predict(m4, newdata = data.frame(pd_post = pd_values$pd), type = "response")
# pd_plot_data <- data.frame(
#   pd = rep(pd_values$pd, 2),
#   predicted_success = c(predictions_pre, predictions_post),
#   time = rep(c("Pre", "Post"), each = nrow(pd_values))
# )
# ggplot(pd_plot_data, aes(x = pd, y = predicted_success, color = time)) +
#   geom_point() +
#   geom_line() +
#   scale_x_continuous(breaks = 1:5) +
#   ylim(0, 1) +
#   labs(
#     title = "Predicted Probability of Success by Perceived Difficulty",
#     x = "Perceived Difficulty",
#     y = "Predicted Probability of Success",
#     color = "Time"
#   ) +
#   theme_minimal()
```


# TRYING TO FIND BEST MODEL (that is not just pd_post alone)
```{r}
# m2 <- glm(success ~ label + pd_pre, data = data_filter, family = binomial)
# m2
# m3 <- glm(success ~ label * pd_pre, data = data_filter, family = binomial)
# m3
# m7 <- glm(success ~ label * group + pd_pre, data = data_filter, family = binomial)
# summary(m7)
# m8 <- glmer(success ~ label * baseline_grade + (1 | ID), data = data, family = binomial)
# m8
# m9 <- glmer(success ~ label * baseline_grade + pd_pre + (1 | ID), data = data, family = binomial)
# m9
m10 <- glm(success ~ label + group + pd_pre + pd_post, data = data_filter, family = binomial)
summary(m10)

AIC(m1, m2, m3, m4, m10)
```

```{r}
# accuracy of m10
data_filter$predicted_success <- predict(m10, type = "response")
data_filter$predicted_class <- ifelse(data_filter$predicted_success > 0.5, 1, 0)
confusion_matrix <- table(data_filter$success, data_filter$predicted_class)
confusion_matrix
accuracy <- sum(diag(confusion_matrix2)) / sum(confusion_matrix2)
accuracy

#inspect misclassifications
data_filter %>%
  filter(success != predicted_class) %>%
  select(ID, label, success, predicted_success, predicted_class, pd_pre, pd_post, group)
```


```{r}
# predicted probabilities and SEs for m10
predictions_m10 <- predict(m10, type = "link", se.fit = TRUE)

# convert to probability scale from log-odds
data_filter$predicted_prob_m10 <- plogis(predictions_m10$fit)
data_filter$predicted_se_m10 <- predictions_m10$se.fit * dlogis(predictions_m10$fit)

# mean predictions and SEs by label
pred_points_m10 <- data_filter %>%
  group_by(label) %>%
  summarize(
    mean_pred_m10 = mean(predicted_prob_m10),
    mean_se_m10 = mean(predicted_se_m10),
    .groups = "drop"
  )

ggplot(data_filter, aes(x = label, y = predicted_prob_m10, color = group)) +
  geom_jitter(width = 0.1, height = 0, alpha = 1, size = 2) +
  geom_point(data = pred_points_m10, aes(x = label, y = mean_pred_m10), 
             color = "black", size = 3) +
  geom_errorbar(data = pred_points_m10, 
                aes(x = label, y = mean_pred_m10, 
                    ymin = mean_pred_m10 - mean_se_m10, 
                    ymax = mean_pred_m10 + mean_se_m10),
                color = "black", width = 0.1, linewidth = 0.5) +

  labs(x = "Label", 
       y = "Predicted probability of success", 
       title = "Probability of Success by Label",
       subtitle = "Model: m10") +
  ylim(0, 1) +
  theme_minimal()
```
```{r}
set.seed(1)
# split to train and test sets
train_indices <- sample(1:nrow(data_filter), size = 0.7 * nrow(data_filter))
train_data <- data_filter[train_indices, ]
test_data <- data_filter[-train_indices, ]
# fit model on train set 
m10_train <- glm(success ~ label + group + pd_pre + pd_post, 
                 data = train_data, family = binomial)
# predict on train set
train_data$predicted_success <- predict(m10_train, newdata = train_data, type = "response")
train_data$predicted_class <- ifelse(train_data$predicted_success > 0.5, 1, 0)
confusion_matrix_train <- table(train_data$success, train_data$predicted_class)
confusion_matrix_train
accuracy_train <- sum(diag(confusion_matrix_train)) / sum(confusion_matrix_train)
accuracy_train
# predict on test set
test_data$predicted_success <- predict(m10_train, newdata = test_data, type = "response")
test_data$predicted_class <- ifelse(test_data$predicted_success > 0.5, 1, 0)
confusion_matrix_test <- table(test_data$success, test_data$predicted_class)
confusion_matrix_test
accuracy_test <- sum(diag(confusion_matrix_test)) / sum(confusion_matrix_test)
accuracy_test
```


